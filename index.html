<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>ABBA</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models</h1>
            <div class="is-size-4 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                  <span>Raghav Singhal<sup>*</sup><sup>1</sup>,</span>
                  <span>Kaustubh Ponkshe<sup>*</sup><sup>1</sup>,</span>
                  <span>Rohit Vartak<sup>*</sup><sup>2</sup>,</span>
              </span>
              <span class="author-block">
                  <a href="https://praneeth.mit.edu/" target="_blank">Praneeth Vepakomma<sup>1,3</sup></a>
              </span>
          </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1 </sup>Mohamed bin Zayed University of Artificial Intelligence, UAE</span>
                    <br><sup>2 </sup>Duke University, USA</span>
                    <br><sup>3 </sup>Massachusetts Institute of Technology, USA</span>
                    <span class="eql-cntrb"><small><br><sup>* </sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         
                      <!-- Arxiv PDF link -->
                      <!-- <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/CERT-Lab/abba" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2505.14238" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
         Your video here -->
<!--         <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large Language Models have demonstrated strong performance across a wide range of tasks, but adapting them efficiently to new domains remains a key challenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by introducing lightweight, trainable modules while keeping most pre-trained weights fixed. The prevailing approach, LoRA, models updates using a low-rank decomposition, but its expressivity is inherently constrained by the rank. Recent methods like HiRA aim to increase expressivity by incorporating a Hadamard product with the frozen weights, but still rely on the structure of the pre-trained model. We introduce <strong>ABBA</strong>, a new PEFT architecture that reparameterizes the update as a Hadamard product of two independently learnable low-rank matrices. In contrast to prior work, ABBA fully decouples the update from the pre-trained weights, enabling both components to be optimized freely. This leads to significantly higher expressivity under the same parameter budget. We formally analyze ABBA's expressive capacity and validate its advantages through matrix reconstruction experiments.  Empirically, ABBA achieves state-of-the-art results on arithmetic and commonsense reasoning benchmarks, consistently outperforming existing PEFT methods by a significant margin across multiple models.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">ABBA Illustration</h2>
    <figure class = "image">
    <img src="static/images/abba_new.png" alt="ABBA Illustration">
      <figcaption class="has-text-justified mt-4">
        LoRA-XS reduces parameter count compared to LoRA by inserting a trainable <em>r × r</em> matrix <em>R</em> between <em>B</em> and <em>A</em>, while keeping other matrices fixed, leading to <em>W = W<sub>0</sub> + sBRA</em>. Our method, LoRA-SB, leverages the same architecture. We find that updating <em>R</em> using its gradients <em>g<sup>R</sup></em> is equivalent to updating the full-finetuning matrix <em>W</em> with an equivalent gradient <em>g<sub>SB</sub> = sBg<sup>R</sup>A</em>. We initialize <em>B</em>, <em>R</em>, and <em>A</em> such that the equivalent gradient <em>g<sub>SB</sub></em> optimally approximates the full fine-tuning gradient <em>g</em> in low rank subspaces <strong>at each training step</strong>. In essence, we simulate the <strong>entire full fine-tuning process</strong> optimally within low-rank subspaces by <strong>utilizing only the initial gradient <em>g<sub>1</sub></em></strong> (shown in green) from full fine-tuning.
      </figcaption>
    </figure>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Contributions</h2>
    <div class="content has-text-justified">
      <ul>
        <li>
          We propose ABBA, a novel PEFT architecture that models the weight update as the Hadamard product of two independently learnable low-rank matrices. This formulation enables highly expressive, high-rank updates while preserving strict parameter efficiency.
        </li>
 
        <li>
          We provide theoretical and empirical analyses of ABBA's expressivity, showing that Hadamard-based decomposition consistently outperforms standard low-rank methods in matrix reconstruction.
        </li>
 
        <li>
          We introduce an exact and efficient reformulation of ABBA using Khatri–Rao factorization, enabling scalable and practical implementation without compromising expressivity.
        </li>

        <li>
          Through extensive experiments on four models across arithmetic and commonsense reasoning tasks, we demonstrate that ABBA achieves state-of-the-art performance, significantly outperforming existing PEFT methods under equal or lower parameter budgets.
        </li>
      </ul>
    </div>
  </div>
 </section>


 <section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centred">Method</h2>
    <div class="content has-text-justified">
      <p>
        Please refer to our paper for a detailed explanation of our method and the associated proofs. We provide a pseudo-code of our algorithm below.
      </p>
    </div>
    <div class="content has-text-centered">
      <img src="static/images/sb-algo.png" alt="LoRA-SB Algorithm">
    </div>
  </div>
 </section>


<section class="section hero-is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Main Results</h2>

    <!-- Carousel wrapper -->
    <div id="fedex-lora-carousel" class="carousel results-carousel">
      <!-- Carousel item 1 -->
      <div class="item">
        <div class="carousel-content">
          <img src="static/images/abba_ar.png" alt="table-math">
          <h2 class="subtitle has-text-centered main-caption">
            Arithmetic Reasoning: Mistral-7B and Gemma-2 9B Comparison of multiple FT methods on Llama-3.2 1B and 3B across eight commonsense reasoning datasets. Best results among PEFT methods are in <strong>bold</strong>.
          </h2>
        </div>
      </div>

      <!-- Carousel item 2 -->
      <div class="item">
        <div class="carousel-content">
          <img src="static/images/sb-cr.png" alt="table-cr">
          <h2 class="subtitle has-text-centered main-caption">
            Commonsense Reasoning: Llama-3.2 3B
          </h2>
        </div>
      </div>

      <!-- Carousel item 3 -->
      <div class="item">
        <div class="carousel-content">
          <img src="static/images/sb-glue.png" alt="table-glue">
          <h2 class="subtitle has-text-centered main-caption">
            GLUE: RoBERTa-large
          </h2>
        </div>
      </div>

      <!-- Carousel item 4 -->
      <div class="item">
        <div class="carousel-content">
          <img src="static/images/loss_mistral_96.png" alt="loss-mistral">
          <h2 class="subtitle has-text-centered main-caption">
            Arithmetic Reasoning: Mistral-7B
          </h2>
        </div>
      </div>

      <!-- Carousel item 5 -->
      <div class="item">
        <div class="carousel-content">
          <img src="static/images/loss_gemma_96.png" alt="loss-gemma">
          <h2 class="subtitle has-text-centered main-caption">
            Arithmetic Reasoning: Gemma-2 9B
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>






<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
       Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> --> 
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
             Your video file here -->
            <!-- <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\ --> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @misc{singhal2025abbahighlyexpressivehadamard,
          title={ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models}, 
          author={Raghav Singhal and Kaustubh Ponkshe and Rohit Vartak and Praneeth Vepakomma},
          year={2025},
          eprint={2505.14238},
          archivePrefix={arXiv},
          primaryClass={cs.CL},
          url={https://arxiv.org/abs/2505.14238}, 
    }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- Default Statcounter code for LoRA SB Project Page
https://raghavsinghal10.github.io/lora-sb-page/ -->
<script type="text/javascript">
  var sc_project=13064177; 
  var sc_invisible=1; 
  var sc_security="71f6139d"; 
  </script>
  <script type="text/javascript"
  src="https://www.statcounter.com/counter/counter.js"
  async></script>
  <noscript><div class="statcounter"><a title="Web Analytics"
  href="https://statcounter.com/" target="_blank"><img
  class="statcounter"
  src="https://c.statcounter.com/13064177/0/71f6139d/1/"
  alt="Web Analytics"
  referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
  <!-- End of Statcounter Code -->
  <!-- End of Statcounter Code -->
  <!-- End of Statcounter Code -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
